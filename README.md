# AfzaAssistant - Advanced Local AI Assistant

A powerful local AI assistant with chat, voice, and image generation capabilities using free, open-source models.

![AfzaAssistant](https://img.shields.io/badge/AI-Assistant-blue) ![Python](https://img.shields.io/badge/Python-3.8+-green) ![Streamlit](https://img.shields.io/badge/Streamlit-Web%20App-red)

## ğŸš€ Features

- **ğŸ’¬ AI Chat** - Powered by Microsoft Phi-3 (3-5x faster than Llama3)
- **ğŸ¤ Voice Chat** - Speech-to-text using OpenAI Whisper
- **ğŸ¨ Image Generation** - Create images with Stable Diffusion
- **ğŸ’» Code Helper** - Programming assistance and code generation
- **ğŸ“ Quick Prompts** - Pre-built prompts for common tasks
- **ğŸ’¾ Chat History** - Persistent conversation memory
- **ğŸ”’ 100% Local** - No data sent to external servers

## ğŸ“‹ Requirements

### System Requirements
- **OS**: Windows 10/11, macOS, or Linux
- **RAM**: 8GB minimum (16GB recommended)
- **Storage**: 10GB free space
- **GPU**: Optional (NVIDIA GPU recommended for faster image generation)

### Software Requirements
- **Python**: 3.8 or higher
- **Ollama**: For running local AI models

## âš¡ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/yourusername/AfzaAssistant.git
cd AfzaAssistant
```

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Install Ollama
**Windows:**
```bash
# Download and install from https://ollama.ai
# Or use PowerShell:
winget install Ollama.Ollama
```

**macOS:**
```bash
brew install ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 4. Install AI Model
```bash
# Start Ollama service
ollama serve

# Install fast Phi-3 model (recommended)
ollama pull phi3
```

### 5. Run Application
```bash
# Easy start (Windows)
start.bat

# Or manually
streamlit run app.py
```

### 6. Open Browser
Navigate to: `http://localhost:8501`

## ğŸ¯ Usage Guide

### Chat Interface
1. Type your question in the main search box
2. Click "AI Chat" for extended conversations
3. Use quick prompts on the right panel for common tasks

### Image Generation
1. Click "Image Generation"
2. Describe your desired image
3. Click "Generate" and wait for creation
4. Images appear instantly in the interface

### Voice Chat
1. Click "Voice Chat"
2. Upload audio file (WAV, MP3, M4A)
3. Click "Process" to transcribe and get AI response

### Code Helper
1. Click "Code Helper"
2. Describe your coding task
3. Get clean, commented code with examples

## ğŸ”§ Configuration

### Switch AI Models
Edit `app.py` line 95 to change models:
```python
response = ollama.chat(model="phi3", messages=[...])
```

### Available Models (Speed Comparison)
```bash
# Recommended (Best balance)
ollama pull phi3              # 3-5x faster than Llama3

# Ultra Fast Options
ollama pull gemma2:2b         # 5-8x faster, 1.6GB
ollama pull qwen2:1.5b        # 8-10x faster, 934MB
ollama pull tinyllama         # 10x+ faster, 637MB

# High Quality Options
ollama pull llama3            # Baseline, 4.7GB
ollama pull mistral           # Good quality, 4.1GB
```

### GPU Acceleration
For faster image generation, ensure CUDA is installed:
```bash
# Check GPU availability
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“ Project Structure
```
AfzaAssistant/
â”œâ”€â”€ app.py              # Main application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ start.bat          # Windows launcher
â”œâ”€â”€ test_app.py        # Testing script
â””â”€â”€ README.md          # This file
```

## ğŸ› ï¸ Troubleshooting

### Common Issues

**1. Ollama Connection Error**
```bash
# Start Ollama service
ollama serve

# Check if running
ollama list
```

**2. Model Not Found**
```bash
# Install the model
ollama pull phi3

# Verify installation
ollama list
```

**3. Port Already in Use**
```bash
# Run on different port
streamlit run app.py --server.port 8502
```

**4. Memory Issues**
- Use smaller models: `gemma2:2b` or `tinyllama`
- Close other applications
- Reduce image generation steps

**5. GPU Not Detected**
- Install CUDA toolkit
- Update GPU drivers
- Restart application

### Performance Optimization

**For Low-End Systems:**
```bash
# Use ultra-fast model
ollama pull gemma2:2b
```

**For High-End Systems:**
```bash
# Use high-quality model
ollama pull llama3
```

## ğŸ”„ Updates

### Update Models
```bash
# Update to latest version
ollama pull phi3

# List all models
ollama list

# Remove old models
ollama rm old_model_name
```

### Update Application
```bash
git pull origin main
pip install -r requirements.txt --upgrade
```

## ğŸ“Š Performance Benchmarks

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| Phi-3 | 2.3GB | â­â­â­â­â­ | â­â­â­â­â­ | Recommended |
| Gemma2:2b | 1.6GB | â­â­â­â­â­ | â­â­â­â­ | Ultra Fast |
| Qwen2:1.5b | 934MB | â­â­â­â­â­ | â­â­â­ | Lightning |
| TinyLlama | 637MB | â­â­â­â­â­ | â­â­ | Testing |
| Llama3 | 4.7GB | â­â­â­ | â­â­â­â­â­ | High Quality |

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch: `git checkout -b feature-name`
3. Commit changes: `git commit -m 'Add feature'`
4. Push to branch: `git push origin feature-name`
5. Submit pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Microsoft** - Phi-3 model
- **OpenAI** - Whisper speech recognition
- **Stability AI** - Stable Diffusion image generation
- **Ollama** - Local model serving
- **Streamlit** - Web interface framework

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/yourusername/AfzaAssistant/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/AfzaAssistant/discussions)
- **Documentation**: [Wiki](https://github.com/yourusername/AfzaAssistant/wiki)

## ğŸ”® Roadmap

- [ ] Multi-language support
- [ ] Plugin system
- [ ] Mobile app
- [ ] Docker deployment
- [ ] API endpoints
- [ ] Custom model training

---

**Made with â¤ï¸ for the AI community**

*Privacy-first â€¢ Open-source â€¢ Lightning-fast*